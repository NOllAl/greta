<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Example models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="greta.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 54px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 59px;
  margin-top: -59px;
}

.section h2 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h3 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h4 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h5 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h6 {
  padding-top: 59px;
  margin-top: -59px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<style type="text/css">
.logo {
    display: inline-block;
    width: 144px;
    height: 40px;
    background-image: url(banner-icon.png);
    background-size: 100% auto;
    background-repeat: no-repeat;
    vertical-align: middle;
}
</style>

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="logo" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav navbar-right">
        <li>
          <a href="get_started.html">get started</a>
        </li>
        <li>
          <a href="example_models.html">examples</a>
        </li>
        <li>
          <a href="reference-index.html">docs</a>
        </li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
            more
            <span class="caret"></span>
          </a>
          <ul class="dropdown-menu" role="menu">
            <li>
              <a href="why_greta.html">why 'greta'?</a>
            </li>
            <li>
              <a href="technical_details.html">technical details</a>
            </li>
            <li>
              <a href="software.html">software</a>
            </li>
            <li>
              <a href="contribute.html">contribute to greta</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="https://github.com/greta-dev/greta">
            <span class="fa fa-github fa-lg"></span>
          </a>
        </li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Example models</h1>

</div>


<div id="common-models" class="section level2">
<h2>Common models</h2>
<p>Below are a few examples of common statistical models implemented in greta.</p>
<hr>
<div id="linear-regression" class="section level3">
<h3>Linear regression</h3>
<p>A simple, one-variable Bayesian linear regression model using the attitude data</p>
<pre class="r"><code># variables &amp; priors
int &lt;- normal(0, 10)
coef &lt;- normal(0, 10)
sd &lt;- cauchy(0, 3, truncation = c(0, Inf))

# linear predictor
mu &lt;- int + coef * attitude$complaints

# observation model
distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="multiple-linear-regression" class="section level3">
<h3>Multiple linear regression</h3>
<p>A multi-variable Bayesian linear regression model using the attitude data</p>
<pre class="r"><code># the predictors as a matrix
design &lt;- as.matrix(attitude[, 2:7])

int &lt;- normal(0, 10)
coefs &lt;- normal(0, 10, dim = ncol(design))
sd &lt;- cauchy(0, 3, truncation = c(0, Inf))

# matrix multiplication is more efficient than multiplying the coefficients
# separately
mu &lt;- int + design %*% coefs

distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="multiple-poisson-regression" class="section level3">
<h3>Multiple Poisson regression</h3>
<p>A multiple Bayesian linear regression model using the <code>warpbreaks</code> data.</p>
<div class="data">
<pre class="r"><code>data(&quot;warpbreaks&quot;)
head(warpbreaks)</code></pre>
<pre><code>  breaks wool tension
1     26    A       L
2     30    A       L
3     54    A       L
4     25    A       L
5     70    A       L
6     52    A       L</code></pre>
<pre class="r"><code>X &lt;- as_data(model.matrix(breaks ~ wool + tension, warpbreaks))
y &lt;- as_data(warpbreaks$breaks)</code></pre>
</div>
<pre class="r"><code>int &lt;- variable()
coefs &lt;- normal(0, 5, dim = ncol(X) - 1)
beta &lt;- c(int, coefs)

eta &lt;- X %*% beta

distribution(y) &lt;- poisson(exp(eta))</code></pre>
<hr>
</div>
<div id="multiple-multinomial-regression" class="section level3">
<h3>Multiple multinomial regression</h3>
<p>A multi-variable Bayesian linear regression model using the iris data.</p>
<div class="data">
<pre class="r"><code>data(iris)

# Only choose a small subset until categorical takes matrix arguments
idxs &lt;- c(1:10, 51:60, 101:111)
n &lt;- length(idxs)

X &lt;- as_data(cbind(1, iris[idxs,1:4]))
y &lt;- model.matrix(~Species, iris[idxs,])
P &lt;- ncol(X)
K &lt;- ncol(y)</code></pre>
</div>
<pre class="r"><code>beta &lt;- normal(0, 5, dim = c(P, K - 1))
eta &lt;- X %*% beta
prob &lt;- imultilogit(eta)

for (i in seq_len(n)) {
  yi &lt;- t(y[i, ])
  distribution(yi) &lt;- categorical(t(prob[i, ]))
}</code></pre>
<hr>
</div>
<div id="multiple-linear-regression-with-lasso-prior" class="section level3">
<h3>Multiple linear regression with LASSO prior</h3>
<p>A multi-variable Bayesian linear regression model using an exponential-normal prior for the coefficients.</p>
<pre class="r"><code># the predictors as a matrix
design &lt;- as.matrix(attitude[, 2:7])

int   &lt;- normal(0, 10)
sd &lt;- cauchy(0, 3, truncation = c(0, Inf))

tau &lt;- exponential(0.5, dim = ncol(design)) 
coefs &lt;- normal(0, tau)
mu &lt;- int + design %*% coefs

distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="hierarchical-linear-regression" class="section level3">
<h3>Hierarchical linear regression</h3>
<p>A hierarchical, Bayesian linear regression model using the iris data, with random intercepts for each of the three species.</p>
<pre class="r"><code># linear model parameters
int &lt;- normal(0, 10)
coef &lt;- normal(0, 10)
sd &lt;- cauchy(0, 3, truncation = c(0, Inf))

# hierarchical model for species effect; use the first species as the baseline
# like in lm()
species_sd &lt;- lognormal(0, 1)
species_offset &lt;- normal(0, species_sd, dim = 2)
species_effect &lt;- rbind(0, species_offset)
species_id &lt;- as.numeric(iris$Species)

# model
mu &lt;- int + coef * iris$Sepal.Width + species_effect[species_id]
distribution(iris$Sepal.Length) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="random-intercept-slope-model" class="section level3">
<h3>Random intercept-slope model</h3>
<p>A hierarchical, Bayesian linear regression model using the iris data, with random intercepts and slopes for each of the three species. The slopes and intercepts for each species are <em>uncorrelated</em> in this example.</p>
<pre class="r"><code># linear model parameters
int &lt;- normal(0, 10)
coef &lt;- normal(0, 10)
sd &lt;- cauchy(0, 3, truncation = c(0, Inf))

species_id &lt;- as.numeric(iris$Species)

# random intercepts
species_int_sd &lt;- lognormal(0, 1)
species_int &lt;- normal(0, species_int_sd, dim = 2)
species_int_eff &lt;- rbind(0, species_int)

# random slopes
species_slope_sd &lt;- lognormal(0, 1)
species_slope &lt;- normal(0, species_slope_sd, dim = 2)
species_slope_eff &lt;- rbind(0, species_slope)

# model
mu &lt;- int + coef * iris$Sepal.Width + species_int_eff[species_id] + iris$Sepal.Width * species_slope_eff[species_id]
distribution(iris$Sepal.Length) &lt;- normal(mu, sd)</code></pre>
</div>
</div>
<div id="common-bayesian-priors" class="section level2">
<h2>Common Bayesian priors</h2>
<p>The following examples show some common Bayesian priors of which some induce sparsity.</p>
<hr>
<div id="improper-flat-prior" class="section level3">
<h3>Improper flat prior</h3>
<p>A simple, one-variable Bayesian linear regression model that uses flat priors for the coefficients. A flat prior using <code>variable</code> puts an unbounded uniform distribution on the parameter. With a flat prior the posterior will be proportional to the likelihood and the MAP will normally correspond to the MLE. Flat priors are usually chosen when there is little knowledge about the parameters available.</p>
<pre class="r"><code># variables &amp; priors
int  &lt;- variable()
coef &lt;- variable()
sd   &lt;- cauchy(0, 3, truncation = c(0, Inf))

# linear predictor
mu &lt;- int + coef * attitude$complaints

# observation model
distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="ridge-prior" class="section level3">
<h3>Ridge prior</h3>
<p>Here we estimate a simple, one-variable Bayesian linear regression model that uses a <em>ridge</em> prior. The ridge prior has a frequentist interpretation where it is used as a penalty for regression coefficients. Among other effects, the penalty shrinks the coefficients towards zero to reduce variance without setting them to zero. The Bayesian version uses a normal distribution for the slopes and a inverse gamma prior for the strength of the penalty. Note that since the prior in our intercept is still improper, the joint prior is also improper.</p>
<pre class="r"><code># variables &amp; priors
int  &lt;- variable()
sd   &lt;- cauchy(0, 3, truncation = c(0, Inf))

tau  &lt;- inverse_gamma(1, 1)
coef &lt;- normal(0, tau)

# linear predictor
mu &lt;- int + coef * attitude$complaints

# observation model
distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="exponential-normal-prior" class="section level3">
<h3>Exponential-normal prior</h3>
<p>In this example we infer the parameters of one-variable Bayesian linear regression model using an exponential-normal prior. A compound exponential-normal prior can be interpreted like an equivalent to the frequentist LASSO. The exponential-normal prior yields a posterior that is pooled towards zero. An exponential-normal prior, or equivalently a Laplace prior, is consequently often chosen when a sparse solution is assumed, which, for instance, is a natural scenario in many biological settings.</p>
<pre class="r"><code># variables &amp; priors
int  &lt;- variable()
sd   &lt;- inverse_gamma(1, 1)

lambda &lt;- gamma(1, 1)
tau  &lt;- exponential(0.5 * lambda**2)
coef &lt;- normal(0, tau)

# linear predictor
mu &lt;- int + coef * attitude$complaints

# observation model
distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="horseshoe-prior" class="section level3">
<h3>Horseshoe prior</h3>
<p>A simple, one-variable Bayesian linear regression model using a horseshoe prior. The horseshoe, just as the LASSO, can be used when the slopes are assumed to be sparse. According to the original <a href="http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf">publication</a> <em>its flat, Cauchy-like tails allow strong signals to remain large […] a posteriori. Yet its infinitely tall spike at the origin provides severe shrinkage for the zero elements of.</em></p>
<pre class="r"><code>horseshoe &lt;- function (tau = 1, dim = NULL) {
  lambda &lt;- cauchy(0, 1, truncation = c(0, Inf), dim = dim)
  sd &lt;- tau ^ 2 * lambda ^ 2
  normal(0, sd, dim = dim)
}

# variables &amp; priors
int  &lt;- variable()
sd   &lt;- inverse_gamma(1, 1)
coef &lt;- horseshoe()

# linear predictor
mu &lt;- int + coef * attitude$complaints

# observation model
distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="regularized-horseshoe-prior" class="section level3">
<h3>Regularized horseshoe prior</h3>
<p>The <a href="https://projecteuclid.org/euclid.ejs/1513306866">regularized (finnished) horseshoe</a> remedies a problem of the original horseshoe: large, unregularized values for the coefficients. This is especially problematic in scenarios where the parameters are only weakly identified by the data, as in logistic regression with perfectly seperable data.</p>
<pre class="r"><code>regularized_horseshoe &lt;- function (tau = 1,  c = 1, dim = NULL) {
  stopifnot(c &gt; 0)
  lambda &lt;- cauchy(0, 1, truncation = c(0, Inf), dim = dim)
  lambda_tilde &lt;- (c^2 * lambda^2) / (c^2 + tau^2 * lambda^2)
  sd &lt;- tau ^ 2 * lambda_tilde ^ 2
  normal(0, sd, dim = dim)
}

# variables &amp; priors
int  &lt;- variable()
sd   &lt;- inverse_gamma(1, 1)
coef &lt;- regularized_horseshoe()

# linear predictor
mu &lt;- int + coef * attitude$complaints

# observation model
distribution(attitude$rating) &lt;- normal(mu, sd)</code></pre>
</div>
</div>
<div id="advanced-bayesian-models" class="section level2">
<h2>Advanced Bayesian models</h2>
<p>Below are some more advanced examples implemented in greta.</p>
<hr>
<div id="hierarchical-linear-regression-in-general-conditional-formulation" class="section level3">
<h3>Hierarchical linear regression in general conditional formulation</h3>
<p>A hierarchical, Bayesian linear regression model using the iris data, with random intercepts and slopes for each of the three species. The slopes and intercepts for each species are <em>correlated</em> in this example. We allow every species to have a species specific slope for <code>Sepal.Length</code>.</p>
<pre class="r"><code>int  &lt;- normal(0, 10)
coef &lt;- normal(0, 10)
sd   &lt;- cauchy(0, 3, truncation = c(0, Inf))

n_species  &lt;- length(unique(iris$Species))
species_id &lt;- as.numeric(iris$Species)

Z &lt;- model.matrix(~ Species + Sepal.Length * Species - 1, data = iris)
gamma &lt;- zeros(n_species * 2)

for (s in unique(species_id)) {
  gamma_sd &lt;- diag(2)
  gamma[c(s, s + n_species)] &lt;- multivariate_normal(rep(0, 2), gamma_sd) 
}

wi &lt;- as_data(iris$Sepal.Width)
Z  &lt;- as_data(Z)
mu &lt;- int + coef * wi + Z %*% gamma

distribution(iris$Sepal.Length) &lt;- normal(mu, sd)</code></pre>
<hr>
</div>
<div id="hierarchical-linear-regression-in-general-marginal-formulation" class="section level3">
<h3>Hierarchical linear regression in general marginal formulation</h3>
<p>A hierarchical, Bayesian linear regression model using the iris data, with random intercepts and slopes for each of the three species. This time we try to set up the <em>marginal</em> model, i.e. when we integrate the conditional density.</p>
<pre class="r"><code>int  &lt;- variable()
coef &lt;- normal(0, 5)
sd   &lt;- cauchy(0, 3, truncation = c(0, Inf))

n_species  &lt;- length(unique(iris$Species))
species_id &lt;- as.numeric(iris$Species)

Z &lt;- model.matrix(~ Species + Sepal.Length * Species - 1, data = iris)
G  &lt;- zeros(n_species * 2, n_species * 2)

for (s in unique(species_id)) {
  G_sd &lt;- diag(2)
  G[c(s, s + n_species), c(s, s + n_species)] &lt;- G_sd
}

mu &lt;- int + coef * iris$Sepal.Width
V &lt;- zeros(nrow(iris), nrow(iris))
diag(V) &lt;- sd

Z &lt;- as_data(Z)
V &lt;- V + Z %*% G %*% t(Z)

sep &lt;- t(iris$Sepal.Width)
distribution(sep) &lt;- multivariate_normal(mu, V)</code></pre>
<hr>
</div>
<div id="bayesian-neural-network" class="section level3">
<h3>Bayesian neural network</h3>
<p><em>Bayesian neural network</em> estimates an easy neural network with a normal prior on the edge weights. For clarity we use an architecture without a hidden layer, such that the weights actually correspond to coefficients in a linear regression model.</p>
<div class="data">
<pre class="text"><code>N &lt;- 100
p &lt;- 10

set.seed(23)  
X &lt;- matrix(rnorm(N * p), N)
beta &lt;- rnorm(10)
y &lt;- X %*% beta + rnorm(N, sd = 0.1)</code></pre>
</div>
<pre class="r"><code>neural_network &lt;- function(x)
{
  # this can be arbitrarily complex, e.g. multiple hidden layers
  x %*% weights
}
  
weights &lt;- normal(0, 1, dim = c(p, 1))
sd &lt;- inverse_gamma(1, 1)

distribution(y) &lt;- normal(neural_network(X), sd)</code></pre>
<pre class="r"><code>m &lt;- model(weights)
d &lt;- mcmc(m)</code></pre>
<pre><code>
    warmup                                    0/1000 | eta:  ?s          
    warmup ==                                50/1000 | eta: 16s          
    warmup ===                              100/1000 | eta: 11s          
    warmup =====                            150/1000 | eta:  9s          
    warmup ======                           200/1000 | eta:  8s          
    warmup ========                         250/1000 | eta:  7s          
    warmup =========                        300/1000 | eta:  6s          
    warmup ===========                      350/1000 | eta:  6s          
    warmup ============                     400/1000 | eta:  5s          
    warmup ==============                   450/1000 | eta:  5s          
    warmup ================                 500/1000 | eta:  4s          
    warmup =================                550/1000 | eta:  4s          
    warmup ===================              600/1000 | eta:  3s          
    warmup ====================             650/1000 | eta:  3s          
    warmup ======================           700/1000 | eta:  2s          
    warmup =======================          750/1000 | eta:  2s          
    warmup =========================        800/1000 | eta:  2s          
    warmup ==========================       850/1000 | eta:  1s          
    warmup ============================     900/1000 | eta:  1s          
    warmup =============================    950/1000 | eta:  0s          
    warmup =============================== 1000/1000 | eta:  0s          

  sampling                                    0/1000 | eta:  ?s          
  sampling ==                                50/1000 | eta:  7s          
  sampling ===                              100/1000 | eta:  7s          
  sampling =====                            150/1000 | eta:  7s          
  sampling ======                           200/1000 | eta:  7s          
  sampling ========                         250/1000 | eta:  7s          
  sampling =========                        300/1000 | eta:  6s          
  sampling ===========                      350/1000 | eta:  6s          
  sampling ============                     400/1000 | eta:  5s          
  sampling ==============                   450/1000 | eta:  5s          
  sampling ================                 500/1000 | eta:  4s          
  sampling =================                550/1000 | eta:  4s          
  sampling ===================              600/1000 | eta:  4s          
  sampling ====================             650/1000 | eta:  3s          
  sampling ======================           700/1000 | eta:  3s          
  sampling =======================          750/1000 | eta:  2s          
  sampling =========================        800/1000 | eta:  2s          
  sampling ==========================       850/1000 | eta:  1s          
  sampling ============================     900/1000 | eta:  1s          
  sampling =============================    950/1000 | eta:  0s          
  sampling =============================== 1000/1000 | eta:  0s          </code></pre>
<pre class="r"><code>print(beta)</code></pre>
<pre><code> [1] -0.73808375  0.34786616 -0.16763053  0.02660026  0.98825290
 [6] -0.50804771  0.28150890 -1.40528527  0.40781010 -1.99132710</code></pre>
<pre class="r"><code>print(summary(d))</code></pre>
<pre><code>
Iterations = 1:1000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 1000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

                  Mean       SD  Naive SE Time-series SE
weights[1,1]  -0.73493 0.010275 0.0003249      0.0005755
weights[2,1]   0.34913 0.011442 0.0003618      0.0006117
weights[3,1]  -0.16444 0.009768 0.0003089      0.0003879
weights[4,1]   0.02861 0.010320 0.0003264      0.0009413
weights[5,1]   0.99296 0.008507 0.0002690      0.0003769
weights[6,1]  -0.51329 0.011615 0.0003673      0.0004117
weights[7,1]   0.28985 0.011152 0.0003527      0.0004126
weights[8,1]  -1.40673 0.010556 0.0003338      0.0006821
weights[9,1]   0.39954 0.011401 0.0003605      0.0004322
weights[10,1] -1.99442 0.011432 0.0003615      0.0004999

2. Quantiles for each variable:

                   2.5%      25%     50%      75%    97.5%
weights[1,1]  -0.755375 -0.74142 -0.7351 -0.72813 -0.71500
weights[2,1]   0.328412  0.34091  0.3490  0.35662  0.37119
weights[3,1]  -0.184588 -0.17094 -0.1641 -0.15828 -0.14294
weights[4,1]   0.008856  0.02215  0.0286  0.03558  0.04867
weights[5,1]   0.976244  0.98741  0.9925  0.99875  1.01083
weights[6,1]  -0.536650 -0.52073 -0.5128 -0.50572 -0.49189
weights[7,1]   0.269269  0.28214  0.2897  0.29696  0.31158
weights[8,1]  -1.426120 -1.41350 -1.4068 -1.39970 -1.38530
weights[9,1]   0.376913  0.39194  0.3999  0.40714  0.42267
weights[10,1] -2.016628 -2.00258 -1.9941 -1.98611 -1.97270</code></pre>
</div>
</div>
<div id="bugs-models" class="section level2">
<h2>BUGS models</h2>
<p>The BUGS project provide a number of example models written in the BUGS modelling language. These models will run in WinBUGS and OpenBUGS, and likely also in JAGS. The <a href="https://github.com/stan-dev/example-models/wiki/BUGS-Examples-Sorted-Alphabetically">Stan wiki</a> provides Stan implementations of these models.</p>
<p>The following sections provide greta implementations of some of these example models, alongside the BUGS code from <a href="https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/WinBUGS_Vol2.pdf">WinBUGS examples volume 2</a> (pdf) and Stan code and an R version of the data from the <a href="https://github.com/stan-dev/example-models/wiki">Stan example models wiki</a>.</p>
<hr>
<div id="air" class="section level3">
<h3>Air</h3>
<p><em>Air</em> analyses reported respiratory illness versus exposure to nitrogen dioxide in 103 children. The parameters <code>alpha</code>, <code>beta</code> and <code>sigma2</code> are known in advance, and the data are grouped into three categories.</p>
<p>See <a href="https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/WinBUGS_Vol2.pdf">WinBUGS examples volume 2</a> (pdf) for details.</p>
<div id="data" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code>y &lt;- c(21, 20, 15)
n &lt;- c(48, 34, 21)
Z &lt;- c(10, 30, 50)
alpha &lt;- 4.48
beta &lt;- 0.76
sigma2 &lt;- 81.14
sigma &lt;- sqrt(sigma2)
tau &lt;- 1 / sigma2
J &lt;- 3</code></pre>
</div>
</div>
<div id="greta-code" class="section level4">
<h4>greta code</h4>
<pre class="r"><code>theta &lt;- normal(0, 32, dim = 2)
mu &lt;- alpha + beta * Z
X &lt;- normal(mu, sigma)
p &lt;- ilogit(theta[1] + theta[2] * X)
distribution(y) &lt;- binomial(n, p)</code></pre>
</div>
<div id="bugsjags-code" class="section level4">
<h4>BUGS/JAGS code</h4>
<div class="bugs">
<pre><code>for(j in 1 : J) {
   y[j] ~ dbin(p[j], n[j])
   logit(p[j]) &lt;- theta[1] + theta[2] * X[j]
   X[j] ~ dnorm(mu[j], tau)
   mu[j] &lt;- alpha + beta * Z[j]
}
theta[1] ~ dnorm(0.0, 0.001)
theta[2] ~ dnorm(0.0, 0.001)</code></pre>
</div>
</div>
<div id="stan-code" class="section level4">
<h4>Stan code</h4>
<div class="stan">
<pre><code>data {
  real alpha; 
  real beta; 
  real&lt;lower=0&gt; sigma2; 
  int&lt;lower=0&gt; J; 
  int y[J]; 
  vector[J] Z;
  int n[J]; 
} 

transformed data {
  real&lt;lower=0&gt; sigma; 
  sigma &lt;- sqrt(sigma2); 
} 

parameters {
   real theta1; 
   real theta2; 
   vector[J] X; 
} 

model {
  real p[J];
  theta1 ~ normal(0, 32);   // 32^2 = 1024 
  theta2 ~ normal(0, 32); 
  X ~ normal(alpha + beta * Z, sigma);
  y ~ binomial_logit(n, theta1 + theta2 * X);
}</code></pre>
</div>
<hr>
</div>
</div>
<div id="beetles" class="section level3">
<h3>Beetles</h3>
<p><em>Beetles</em> considers dose-response data from an experiment applying carbon disulphide to 8 beetles. The original example compares three different link functions; the logit, probit and complementary log-log. Here, only the code for the logit link is shown. You can implement the other two link functions in greta by changing <code>ilogit</code> to <code>iprobit</code> or <code>icloglog</code>.</p>
<p>See <a href="https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/WinBUGS_Vol2.pdf">WinBUGS examples volume 2</a> (pdf) for details.</p>
<div id="data-1" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code>x &lt;- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
n &lt;- c(59, 60, 62, 56, 63, 59, 62, 60)
r &lt;- c(6, 13, 18, 28, 52, 53, 61, 60)
N &lt;- 8</code></pre>
</div>
</div>
<div id="greta-code-1" class="section level4">
<h4>greta code</h4>
<pre class="r"><code>alpha_star &lt;- normal(0, 32)
beta &lt;- normal(0, 32)
p &lt;- ilogit(alpha_star + beta * (x - mean(x)))
distribution(r) &lt;- binomial(n, p)

alpha &lt;- alpha_star - beta * mean(x)
rhat &lt;- p * n</code></pre>
</div>
<div id="bugsjags-code-1" class="section level4">
<h4>BUGS/JAGS code</h4>
<div class="bugs">
<pre><code>for( i in 1 : N ) {
  r[i] ~ dbin(p[i],n[i])
  logit(p[i]) &lt;- alpha.star + beta * (x[i] - mean(x[]))
  rhat[i] &lt;- n[i] * p[i]
  culmative.r[i] &lt;- culmative(r[i], r[i])
}
alpha &lt;- alpha.star - beta * mean(x[])
beta ~ dnorm(0.0,0.001)
alpha.star ~ dnorm(0.0,0.001)</code></pre>
</div>
</div>
<div id="stan-code-1" class="section level4">
<h4>Stan code</h4>
<div class="stan">
<pre><code>data {
    int&lt;lower=0&gt; N;
    int&lt;lower=0&gt; n[N];
    int&lt;lower=0&gt; r[N];
    vector[N] x;
}

transformed data {
    vector[N] centered_x;
    real mean_x;
    mean_x &lt;- mean(x);
    centered_x &lt;- x - mean_x;
}

parameters {
    real alpha_star;
    real beta;
}

transformed parameters {
    vector[N] m;
    m &lt;- alpha_star + beta * centered_x;
}

model {
  alpha_star ~ normal(0.0, 1.0E4);  
  beta ~ normal(0.0, 1.0E4);
  r ~ binomial_logit(n, m);
}

generated quantities {
  real alpha; 
  real p[N];
  real llike[N];
  real rhat[N];
  for (i in 1:N)  {
    p[i] &lt;- inv_logit(m[i]);
    llike[i]  &lt;- r[i]*log(p[i]) + (n[i]-r[i])*log(1-p[i]);  
    rhat[i] &lt;- p[i]*n[i];  // fitted values
  }
  alpha &lt;- alpha_star - beta*mean_x;              
} </code></pre>
</div>
</div>
</div>
</div>
<div id="stan-models" class="section level2">
<h2>Stan models</h2>
<p>The following few code examples show how Stan code can be translated in equivalent greta models.</p>
<hr>
<div id="lightspeed" class="section level3">
<h3>Lightspeed</h3>
<p><em>Lightspeed</em> estimates a linear normal model without predictors. The data are 66 measurements from Simon Newcomb and represent the time required for light to travel roughly 7500 meters.</p>
<p>See also the <a href="https://github.com/stan-dev/example-models/wiki/ARM-Models-Sorted-by-Type#no-predictors">Stan examples</a> for details.</p>
<div id="data-2" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code>y &lt;- c(28, 26, 33, 24, 34, -44, 27, 16, 40, -2, 29, 22, 24, 21, 25, 
       30, 23, 29, 31, 19, 24, 20, 36, 32, 36, 28, 25, 21, 28, 29, 
       37, 25, 28, 26, 30, 32, 36, 26, 30, 22, 36, 23, 27, 27, 28, 
       27, 31, 27, 26, 33, 26, 32, 32, 24, 39, 28, 24, 25, 32, 25, 
       29, 27, 28, 29, 16, 23)
n &lt;- length(y)</code></pre>
</div>
</div>
<div id="greta-code-2" class="section level4">
<h4>greta code</h4>
<pre class="r"><code>beta  &lt;- variable()
sigma &lt;- variable(lower = 0)

distribution(y) &lt;- normal(beta, sigma)</code></pre>
</div>
<div id="stan-code-2" class="section level4">
<h4>Stan code</h4>
<div class="stan">
<pre><code>data {
  int&lt;lower=0&gt; N; 
  vector[N] y;
}
parameters {
  vector[1] beta;
  real&lt;lower=0&gt; sigma;
} 
model {
  y ~ normal(beta[1],sigma);
}</code></pre>
</div>
<hr>
</div>
</div>
<div id="eight-schools" class="section level3">
<h3>Eight schools</h3>
<p><em>Eight schools</em> estimates the effect of coaching programs in eight schools. The data are 8 measurements of coaching effects along with their standard errors.</p>
<p>See also the <a href="https://github.com/stan-dev/example-models/wiki/ARM-Models-Sorted-by-Type#varying-intercept">Stan example</a> for details.</p>
<div id="data-3" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code>y       &lt;- c(28,  8, -3,  7, -1,  1, 18, 12)
sigma_y &lt;- c(15, 10, 16, 11,  9, 11, 10, 18)
N       &lt;- length(y)</code></pre>
</div>
</div>
<div id="greta-code-3" class="section level4">
<h4>greta code</h4>
<pre class="r"><code>sigma_eta &lt;- inverse_gamma(1, 1)
eta       &lt;- normal(0, sigma_eta, dim=N)

mu_theta &lt;- normal(0, 100)
xi       &lt;- normal(0, 5)
theta    &lt;- mu_theta + xi * eta

distribution(y) &lt;- normal(theta, sigma_y)</code></pre>
</div>
<div id="stan-code-3" class="section level4">
<h4>Stan code</h4>
<div class="stan">
<pre><code>data {
  int&lt;lower=0&gt; N;
  vector[N] y;
  vector[N] sigma_y;
} 
parameters {
  vector[N] eta;
  real mu_theta;
  real&lt;lower=0,upper=100&gt; sigma_eta;
  real xi;
} 
transformed parameters {
  real&lt;lower=0&gt; sigma_theta;
  vector[N] theta;

  theta = mu_theta + xi * eta;
  sigma_theta = fabs(xi) / sigma_eta;
}
model {
  mu_theta ~ normal(0, 100);
  sigma_eta ~ inv_gamma(1, 1); //prior distribution can be changed to uniform

  eta ~ normal(0, sigma_eta);
  xi ~ normal(0, 5);
  y ~ normal(theta,sigma_y);
}</code></pre>
</div>
</div>
</div>
</div>
<div id="ecological-models" class="section level2">
<h2>Ecological models</h2>
<p>Here we provide some examples of common ecological models. We begin with a basic logistic regression often used in species distribution modelling to estimate species probability of presence. We then provide increasingly complex species distribution models, beginning with modelling obervation error directly, and moving on to models for multiple species: independentally but concurrently modelled species, partially pooled coefficients, repeated measures, and sub-models.</p>
<p>One important way <code>greta</code> models differs from BUGS and Stan code is that it is often neccessary to set up the linear predictor seperatly to the logistic transformation. This is particulary the case when the dimensions of the data increases, such as when there are multiple species being modelled. We keep this convention in the simpler models for ease of understanding.</p>
<hr>
<div id="logistic-regression" class="section level3">
<h3>Logistic regression</h3>
<p><em>Logistic regression</em> This is an example of a simple logistic regression being used to estimate the probability of species presence along a number of environmental gradients. We first simulate some data to model followed by the <code>greta</code> code.</p>
<div id="data-4" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code># make fake data
n_env &lt;- 3
n_sites &lt;- 20

# n_sites x n_env matrix of environmental variables
env &lt;- matrix(rnorm(n_sites * n_env), nrow = n_sites) 
# n_sites observations of species presence or absence
occupancy &lt;- rbinom(n_sites, 1, 0.5) </code></pre>
</div>
</div>
<div id="greta-code-4" class="section level4">
<h4>greta code</h4>
<pre class="r"><code># load greta
library(greta)

# create matrices to greta arrays
X &lt;- as_data(env)
Y &lt;- as_data(occupancy)

# create greta arrays for random variables
alpha &lt;- normal(0, 10)
beta &lt;- normal(0, 10, dim = n_env)

# logit-linear model
linear_predictor &lt;- alpha + X %*% beta
p &lt;- ilogit(linear_predictor)

# distribution (likelihood) over observed values
distribution(Y) &lt;- bernoulli(p)</code></pre>
<hr>
</div>
</div>
<div id="poisson-regression" class="section level3">
<h3>Poisson regression</h3>
<p>An example of a simple poisson regression being used to estimate the abundance of a species along a number of environmental gradients. We first simulate some data to model followed by the <code>greta</code> code.</p>
<div id="data-5" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code># make fake data
n_env &lt;- 3
n_sites &lt;- 20

# n_sites x n_env matrix of environmental variables
env &lt;- matrix(rnorm(n_sites * n_env), nrow = n_sites) 
# n_sites observations of species abundance
occupancy &lt;- rpois(n_sites, 5) </code></pre>
</div>
</div>
<div id="greta-code-5" class="section level4">
<h4>greta code</h4>
<pre class="r"><code># load greta
library(greta)

# create matrices to greta arrays
X &lt;- as_data(env)
Y &lt;- as_data(occupancy)

# create greta arrays for random variables
alpha &lt;- normal(0, 10)
beta &lt;- normal(0, 10, dim = n_env)

# model
linear_predictor &lt;- alpha + X %*% beta
lambda &lt;- exp(linear_predictor)
distribution(Y) &lt;- poisson(lambda)</code></pre>
<hr>
</div>
</div>
<div id="logistic-regression-with-error-term" class="section level3">
<h3>Logistic regression with error term</h3>
<p><em>Logistic regression with observational error term</em> This is an example of a simple logistic regression with an extra observation-level error term We first simulate some data to model followed by the <code>greta</code> code.</p>
<div id="data-6" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code># make fake data
n_env &lt;- 3
n_sites &lt;- 20

# n_sites x n_env matrix of environmental variables
env &lt;- matrix(rnorm(n_sites * n_env), nrow = n_sites) 
# n_sites observations of species presence or absence
occupancy &lt;- rbinom(n_sites, 1, 0.5)</code></pre>
</div>
</div>
<div id="greta-code-6" class="section level4">
<h4>greta code</h4>
<pre class="r"><code># load greta
library(greta)

# create matrices to greta arrays
X &lt;- as_data(env)
Y &lt;- as_data(occupancy)

# create greta arrays for random variables
alpha &lt;- normal(0, 10)
beta &lt;- normal(0, 10, dim = n_env)
error &lt;- normal(0, 10, dim = n_sites)

# logit-linear model with extra variation
linear_predictor &lt;- alpha + X %*% beta + error
p &lt;- ilogit(linear_predictor)

# distribution (likelihood) over observed values
distribution(Y) &lt;- bernoulli(p)</code></pre>
<hr>
</div>
</div>
<div id="multiple-species-modelling-independently-and-concurrently" class="section level3">
<h3>Multiple species modelling independently and concurrently</h3>
<p>An example of a logistic regression being used to estimate the probability of multiple species’ presences along a number of environmental gradients. Although modelled concurrently, the random variables for each species are independent. We first simulate some data to model followed by the <code>greta</code> code.</p>
<p>Where a single observation per species and location would have a bernoulli error distribution, multiple observations for each species and location have a binomial distribution. The small change in the code is commented below.</p>
<p>When modelling multiple species (or other grouping factor), we need an extra step in constructing the linear predictor. In order to add multiple <code>greta</code> arrays together <em>for each species</em> we can use the <code>sweep()</code> function.</p>
<div id="data-7" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code># make fake data
n_species &lt;- 5
n_env &lt;- 3
n_sites &lt;- 20

env &lt;- matrix(rnorm(n_sites * n_env), nrow = n_sites)
occupancy &lt;- matrix(rbinom(n_species * n_sites, 1, 0.5), nrow = n_sites)</code></pre>
</div>
</div>
<div id="greta-code-7" class="section level4">
<h4>greta code</h4>
<pre class="r"><code># load greta
library(greta)

# create matrices to greta arrays
X &lt;- as_data(env)
Y &lt;- as_data(occupancy)

# variables
alpha &lt;- normal(0,10, dim = n_species)
beta &lt;- normal(0, 10, dim = c(n_env, n_species))

env_effect &lt;- X %*% beta

# matrix addition with `sweep()` create interim variable
linear_predictor &lt;- sweep(env_effect, 2, alpha, FUN = &#39;+&#39;)

# ilogit of linear predictor
p &lt;- ilogit(linear_predictor)

# a single observation means our data are bernoulli distributed
distribution(Y) &lt;- bernoulli(p)
# for n_obs repeat observations per species/location, we could use the binomial
# distribution:
# distribution(Y) &lt;- binomial(n_obs, p) </code></pre>
<hr>
</div>
</div>
<div id="multiple-species-with-partial-pooling-of-regression-coefficients" class="section level3">
<h3>Multiple species with partial pooling of regression coefficients</h3>
<p>An example of a logistic regression being used to estimate the probability of multiple species’ presences along a number of environmental gradients. Instead of assuming independence of species regression coefficients, we assume they are drawn from a shared distribution. We partially pool species responses. This gives us not ony the regression coefficients for each species but also a global average coefficient and a measure of variation between species responses to environmental gradients.</p>
<p>We first simulate some data to model followed by the <code>greta</code> code.</p>
<div id="data-8" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code># make fake data
n_species &lt;- 5
n_env &lt;- 1
n_sites &lt;- 50

env &lt;- matrix(rnorm(n_sites * n_env), nrow = n_sites)
occupancy &lt;- matrix(rbinom(n_sites * n_species, 1, 0.5), nrow = n_sites)</code></pre>
</div>
</div>
<div id="greta-code-8" class="section level4">
<h4>greta code</h4>
<pre class="r"><code># load greta
library(greta)

# create matrices to greta arrays
X &lt;- as_data(env)
Y &lt;- as_data(occupancy)

# variables
global_alpha &lt;- normal(0, 10, dim = 1)
global_alpha_sd &lt;- uniform(0, 10, dim = 1) 
alpha &lt;- normal(global_alpha, global_alpha_sd, dim = n_species)

global_betas &lt;- normal(0, 10, dim = n_env)
global_betas_sd &lt;- uniform(0, 10, dim = n_env)
beta &lt;- normal(global_betas, global_betas_sd, dim = c(n_env, n_species))

env_effect &lt;- X %*% beta

# matrix addition with `sweep()` create interim variable
linear_predictor &lt;- sweep(env_effect, 2, alpha, FUN = &#39;+&#39;)

# ilogit of linear predictor
p &lt;- ilogit(linear_predictor)

# a single observation means our data are bernoulli distributed
distribution(Y) &lt;- bernoulli(p)</code></pre>
<hr>
</div>
</div>
<div id="multiple-species-with-sub-model-for-regression-coefficients" class="section level3">
<h3>Multiple species with sub-model for regression coefficients</h3>
<p>An example of a logistic regression being used to estimate the probability of multiple species’ presences along a number of environmental gradients. Instead of assuming independence of species regression coefficients, or partial pooling in shared distributions, we use a sub-model to estimate species regression coefficients. In this case, we’re using species traits to estimate their response to different environmental gradients.</p>
<p>Because we’re building a sub-model, it’s more efficient to simply add a coloumn of ones to dataframes for the base model and sub-model. This is simply to prevent our code from becoming too cumbersome. If we didn’t want to use our sub-model to estimate the intercept, we would not need to include the column of ones in the environmental dataframe.</p>
<p>We first simulate some data to model followed by the <code>greta</code> code.</p>
<div id="data-9" class="section level4">
<h4>data</h4>
<div class="data">
<pre class="text"><code># make fake data
n_species &lt;- 3
n_env &lt;- 1
n_sites &lt;- 5
n_traits &lt;- 1

# n_sites x n_env matrix of environmental variables
env &lt;- matrix(rnorm(n_sites * n_env), nrow = n_sites)
# n_species * n_traits matix of trait variables
traits &lt;- matrix(rnorm(n_species * n_traits), nrow = n_species)
# n_sites * n_species matrix of observed occupancy
occupancy &lt;- matrix(rbinom(n_sites * n_species, 1, 0.5), nrow = n_sites)</code></pre>
</div>
</div>
<div id="greta-code-9" class="section level4">
<h4>greta code</h4>
<pre class="r"><code># load greta
library(greta)

# data wrangling

# include a column of 1&#39;s for intercept estimation in the sub-model (traits) and base model
traits &lt;- cbind(rep(1, n_species), traits)
env &lt;- cbind(rep(1, n_sites), env)

# redefine the n_env and n_traits after adding in coloum of 1&#39;s for intercepts
n_env &lt;- ncol(env)
n_traits &lt;- ncol(traits)

# create matrices to greta arrays
X &lt;- as_data(env)
Y &lt;- as_data(occupancy)
U &lt;- as_data(traits)

# greta arrays for variables to be estimated
# sub-model parameters have normal prior distributions
g &lt;- normal(0, 10, dim = c(n_env, n_traits))
# parameters of the base model are a function of the parameters of the sub-model
beta &lt;-  g %*% t(U) 

# use the coefficients to get the model linear predictor
linear_predictor &lt;- X %*% beta 

# use the logit link to get probabilities of occupancy
p &lt;- ilogit(linear_predictor)

# data are bernoulli distributed
distribution(Y) &lt;- bernoulli(p)</code></pre>
<hr>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
